{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3af4b6-620f-49f3-9303-d356293d733b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e2fc7d-cbac-4f9c-ae58-f814099181ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='gemma:2b')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"gemma:2b\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f7fc5-1a13-4c96-ae83-1ca0cdcd9f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12132b85-0dcd-4d99-84cd-23fc4000e45c",
   "metadata": {},
   "source": [
    "***building a RunnableSequence***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd4838c-3a29-479c-90af-8c3e0d922335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='\\n        write a joke on the topic: {topic}\\n    ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        write a joke on the topic: {topic}\n",
    "    \"\"\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c9f9a2-17c8-4fec-bd74-2fa29d218cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f5ae9f-ed18-4f2b-b7f4-977af6188836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ask here... AI\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"ask here...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28540a3-cf76-4ca2-a992-5914035aa261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you call an AI that's too smart?\n",
      "\n",
      "... A sentient chatbot.\n"
     ]
    }
   ],
   "source": [
    "# creating a simple sequence\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "chain = RunnableSequence(prompt1, model, parser)\n",
    "\n",
    "response = chain.invoke({\"topic\": user_input})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccfb47-29aa-4ced-a960-4add4e828e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "624a51b6-18d7-43ea-9d95-8dc66a5896c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='\\n        Take the joke: {text}, and explain the sarcism behind it in simple and short way!\\n    ')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 = PromptTemplate(\n",
    "    template = \"\"\"\n",
    "        Take the joke: {text}, and explain the sarcism behind it in simple and short way!\n",
    "    \"\"\",\n",
    "    input_variables= ['text']\n",
    ")\n",
    "\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384413d-63a8-4804-b700-6d43a6228052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90584f0b-f89b-4714-b9f9-b00707eb9f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sarcasm behind the joke is that the AI is deliberately choosing not to make a decision, which is a form of indecisiveness. The AI is playing on the word \"indecisive\", which means lacking the ability to make a decision or choice. The humor arises from the contrast between the AI's literal lack of decision and its sarcastic response.\n"
     ]
    }
   ],
   "source": [
    "# more lenghty sequence\n",
    "\n",
    "chain_2 = RunnableSequence(prompt1, model, parser, prompt2, model, parser)\n",
    "\n",
    "response2 = chain_2.invoke({'topic':user_input})\n",
    "\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9267073-3204-4078-8b9f-a80c55c298c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fba7da11-0ca9-4ef6-83a7-65849ecdeced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOllama |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOllama |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain_2.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a48da-7a7f-414c-9010-738b693072b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
